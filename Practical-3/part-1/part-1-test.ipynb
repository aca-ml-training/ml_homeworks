{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import decision_tree_helper\n",
    "from decision_tree_helper import build_tree\n",
    "\n",
    "class DecisionTree(object):\n",
    "    \"\"\"\n",
    "    DecisionTree class, that represents one Decision Tree\n",
    "\n",
    "    :param max_tree_depth: maximum depth for this tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_tree_depth):\n",
    "        self.max_depth = max_tree_depth\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param Y: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \"\"\"\n",
    "        data = X.tolist()\n",
    "        for index in range(len(X)):\n",
    "            data[index].append(Y[index])\n",
    "        self.tree = build_tree(data, max_depth = self.max_depth)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :return: Y - 1 dimension python list with labels\n",
    "        \"\"\"\n",
    "        X = X.tolist()\n",
    "        Y = []\n",
    "        for row in X:\n",
    "            current_node = self.tree     \n",
    "            while(current_node != None and not current_node.is_leaf):\n",
    "                feature_val = row[current_node.column] \n",
    "                if type(feature_val) == int or type(feature_val) == float:\n",
    "                    if feature_val >= current_node.value:\n",
    "                        current_node = current_node.true_branch\n",
    "                    else: \n",
    "                        current_node = current_node.false_branch\n",
    "                if type(feature_val) == str:\n",
    "                    if feature_val == current_node.value:\n",
    "                        current_node = current_node.true_branch\n",
    "                    else:\n",
    "                        current_node = current_node.false_branch\n",
    "            Y.append(current_node.result)\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from decision_tree import DecisionTree\n",
    "\n",
    "class RandomForest(object):\n",
    "    \"\"\"\n",
    "    RandomForest a class, that represents Random Forests.\n",
    "\n",
    "    :param num_trees: Number of trees in the random forest\n",
    "    :param max_tree_depth: maximum depth for each of the trees in the forest.\n",
    "    :param ratio_per_tree: ratio of points to use to train each of\n",
    "        the trees.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_trees, max_tree_depth, ratio_per_tree=0.5):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_tree_depth = max_tree_depth\n",
    "        self.trees = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param Y: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        ind = np.arange(X.shape[0])\n",
    "        for _ in rang(num_trees):\n",
    "            train_ind = np.random.choice(ind, int(X.shape[0]*ratio_per_tree), replace=False)\n",
    "            tree_clf = DecisionTree(max_tree_depth)\n",
    "            tree_clf.fit(X[train_ind], Y[train_ind])\n",
    "            self.trees.append(tree_clf.tree)\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :return: (Y, conf), tuple with Y being 1 dimension python\n",
    "        list with labels, and conf being 1 dimensional list with\n",
    "        confidences for each of the labels.\n",
    "        \"\"\"\n",
    "        Y_list = []\n",
    "        for tree in self.trees:\n",
    "            Y_list.append(tree.predict(X))\n",
    "        Y = []\n",
    "        conf = []\n",
    "        for i in range(self.num_trees):\n",
    "            Y_i_counter = get_columns(Y_list, i)\n",
    "            y = max(Y_i_counter, key=lambda key: dict_current_results[key])\n",
    "            Y.append(y)\n",
    "            conf.append(Y_i_counter[y]/self.num_trees)\n",
    "        return (Y, conf)\n",
    "    \n",
    "    \n",
    "def get_columns(list_2D, cols):\n",
    "    if type(cols) == int or type(cols) == float:\n",
    "        return [item[cols] for item in list_2D]\n",
    "    return [[item[col] for col in cols] for item in list_2D]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration= 0, and cost_value= 1.4505570899066784\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4505570899066784\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4555079080832756\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.445606271730081\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4357046353768863\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4357046353768865\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4357046353768863\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4357046353768865\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.445606271730081\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.46540954443647\n",
      "the gradient descent algorithm is finished.\n",
      "Decision Tree Accuracy =  0.742307692308  ( 0.0 )\n",
      "Random Forest Tree Accuracy =  0.0  ( 0.0 )\n",
      "Logistic Reg. Accuracy =  0.807692307692  ( 0.0 )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from logistic_regression import LogisticRegression\n",
    "from decision_tree import DecisionTree\n",
    "\n",
    "def accuracy_score(Y_true, Y_predict):\n",
    "    tp_plus_tn = 0\n",
    "    N = len(Y_true)\n",
    "    for index in range(N):\n",
    "        if Y_true[index] == Y_predict[index]:\n",
    "            tp_plus_tn += 1\n",
    "    return tp_plus_tn / N\n",
    "\n",
    "\n",
    "def evaluate_performance():\n",
    "    '''\n",
    "    Evaluate the performance of decision trees and logistic regression,\n",
    "    average over 1,000 trials of 10-fold cross validation\n",
    "\n",
    "    Return:\n",
    "      a matrix giving the performance that will contain the following entries:\n",
    "      stats[0,0] = mean accuracy of decision tree\n",
    "      stats[0,1] = std deviation of decision tree accuracy\n",
    "      stats[1,0] = mean accuracy of logistic regression\n",
    "      stats[1,1] = std deviation of logistic regression accuracy\n",
    "\n",
    "    ** Note that your implementation must follow this API**\n",
    "    '''\n",
    "\n",
    "    # Load Data\n",
    "    filename = 'data/SPECTF.dat'\n",
    "    data = np.loadtxt(filename, delimiter=',')\n",
    "    X = data[:, 1:]\n",
    "    y = np.array(data[:, 0])\n",
    "    n, d = X.shape\n",
    "\n",
    "    all_accuracies_dt = []\n",
    "    all_accuracies_lr = []\n",
    "    for trial in range(1):\n",
    "        idx = np.arange(n)\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "        \n",
    "        ind = np.arange(X.shape[0])\n",
    "        #classifier_dt = tree.DecisionTreeClassifier(max_depth=50)\n",
    "        classifier_lr = LogisticRegression(max_steps=10000, epsilon=1e-7)\n",
    "        scores_dt = []\n",
    "        scores_lr = []\n",
    "        for i in range(10):\n",
    "            test_ind = np.random.choice(ind, int(X.shape[0]/10), replace=False)\n",
    "            ind = np.setdiff1d(np.arange(X.shape[0]), test_ind)\n",
    "            X_train, Y_train, X_test, Y_test = X[ind], y[ind], X[test_ind], y[test_ind]\n",
    "            # train the decision tree\n",
    "            classifier_dt.fit(X_train, Y_train)\n",
    "            accuracy_dt = accuracy_score(Y_true=Y_test, Y_predict=classifier_dt.predict(X_test))\n",
    "            scores_dt.append(accuracy_dt)\n",
    "            classifier_lr.fit(X_train, Y_train)\n",
    "            accuracy_lr = accuracy_score(Y_true=Y_test, Y_predict=classifier_lr.predict(X_test))\n",
    "            scores_lr.append(accuracy_lr)\n",
    "        all_accuracies_dt.append(np.mean(scores_dt))\n",
    "        all_accuracies_lr.append(np.mean(scores_lr))\n",
    "       \n",
    "\n",
    "\n",
    "    # compute the training accuracy of the model\n",
    "    meanDecisionTreeAccuracy = np.mean(all_accuracies_dt)\n",
    "    stddevDecisionTreeAccuracy = np.std(all_accuracies_dt)\n",
    "    # TODO: update these statistics based on the results of your experiment\n",
    "    meanLogisticRegressionAccuracy = np.mean(all_accuracies_lr)\n",
    "    stddevLogisticRegressionAccuracy = np.std(all_accuracies_lr)\n",
    "    meanRandomForestAccuracy = 0\n",
    "    stddevRandomForestAccuracy = 0\n",
    "\n",
    "    # make certain that the return value matches the API specification\n",
    "    stats = np.zeros((3, 2))\n",
    "    stats[0, 0] = meanDecisionTreeAccuracy\n",
    "    stats[0, 1] = stddevDecisionTreeAccuracy\n",
    "    stats[1, 0] = meanRandomForestAccuracy\n",
    "    stats[1, 1] = stddevRandomForestAccuracy\n",
    "    stats[2, 0] = meanLogisticRegressionAccuracy\n",
    "    stats[2, 1] = stddevLogisticRegressionAccuracy\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Do not modify from HERE...\n",
    "if __name__ == \"__main__\":\n",
    "    stats = evaluate_performance()\n",
    "    print(\"Decision Tree Accuracy = \", stats[0, 0], \" (\", stats[0, 1], \")\")\n",
    "    print(\"Random Forest Tree Accuracy = \", stats[1, 0], \" (\", stats[1, 1], \")\")\n",
    "    print(\"Logistic Reg. Accuracy = \", stats[2, 0], \" (\", stats[2, 1], \")\")\n",
    "# ...to HERE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \n",
    "    def __init__(self, epsilon=0.0001, l=1, step_size=0.01, max_steps=1000, initial_beta=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.l = l\n",
    "        self.step_size = step_size\n",
    "        self.max_steps = max_steps\n",
    "        self.initial_beta = initial_beta\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param Y: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \"\"\"\n",
    "        self.beta = stochastic_gradient_descent(X = X, \n",
    "                                                Y = Y, \n",
    "                                                epsilon=self.epsilon, \n",
    "                                                l=self.l, \n",
    "                                                step_size=self.step_size, \n",
    "                                                max_steps=self.max_steps,\n",
    "                                               initial_beta=self.initial_beta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :return: Y - 1 dimension python list with labels\n",
    "        \"\"\"\n",
    "        Y = []\n",
    "        for row in X:\n",
    "            Y.append(int(np.dot(row, beta) >= 0))\n",
    "        return Y\n",
    "\n",
    "def sigmoid(s):\n",
    "    return 1/(1+np.exp(-s))\n",
    "\n",
    "def cost_function(X, Y, beta):\n",
    "    h = sigmoid(np.dot(X, beta))\n",
    "    return (-np.dot(Y, np.log(h))+ np.dot((1+Y),1-h))/X.shape[0]\n",
    "\n",
    "def stochastic_gradient_descent(X, Y, epsilon=0.0001, l=1, step_size=0.01, max_steps=1000, initial_beta=None):\n",
    "    \"\"\"\n",
    "    Implement gradient descent using stochastic approximation of the gradient.\n",
    "    :param X: data matrix (2 dimensional np.array)\n",
    "    :param Y: response variables (1 dimensional np.array)\n",
    "    :param l: regularization parameter lambda\n",
    "    :param epsilon: approximation strength\n",
    "    :param max_steps: maximum number of iterations before algorithm will\n",
    "        terminate.\n",
    "    :return: value of beta (1 dimensional np.array)\n",
    "    \"\"\"\n",
    "    N, D = X.shape[0], X.shape[1]\n",
    "    X, l_vector, var_cols, std_cols, mean_cols = parameters_for_scaling(X, l)\n",
    "    l_vector[0] = 0\n",
    "    if initial_beta == None:\n",
    "        beta = np.zeros(D)\n",
    "    else:\n",
    "        beta = initial_beta\n",
    "    for s in range(max_steps):\n",
    "        if s % N == 0:\n",
    "            X, Y = shuffle_data(X, Y)\n",
    "        next_beta = beta - step_size*normalized_gradient(X[s%N], Y[s%N], beta, l_vector)\n",
    "        if s % 1000 == 0:\n",
    "            print('iteration= {}, and cost_value= {}'.format(s, cost_function(X, Y, beta)))\n",
    "        if np.linalg.norm(next_beta - beta)/np.linalg.norm(next_beta) < epsilon:\n",
    "            print('the gradient descent algorithm is finished.')\n",
    "            return get_real_beta(next_beta, std_cols, mean_cols)\n",
    "        beta = next_beta\n",
    "    print('the gradient descent algorithm is finished.')\n",
    "    return get_real_beta(beta, std_cols, mean_cols)\n",
    "\n",
    "\n",
    "\n",
    "def normalized_gradient(X, Y, beta, l):\n",
    "    \"\"\"\n",
    "    :param X: data matrix (2 dimensional np.array)\n",
    "    :param Y: response variables (1 dimensional np.array)\n",
    "    :param beta: value of beta (1 dimensional np.array)\n",
    "    :param l: regularization parameter lambda\n",
    "    :return: normalized gradient, i.e. gradient normalized according to data\n",
    "    \"\"\"\n",
    "    return (np.dot(X.T, sigmoid(np.dot(X,beta))-Y) + l*beta/2)/X.shape[0]\n",
    "\n",
    "def parameters_for_scaling(X, l):\n",
    "    X = np.copy(X)\n",
    "    featchures_mat = X[:, 1:]\n",
    "    var_cols = np.var(X, axis=0)\n",
    "    std_cols = np.std(X, axis=0)\n",
    "    mean_cols = np.mean(X, axis=0)\n",
    "    var_cols[0] = 1\n",
    "    featchures_mat = X[:, 1:]\n",
    "    X[:, 1:] = div0(featchures_mat - np.mean(featchures_mat, axis=0),np.std(featchures_mat, axis=0))\n",
    "    return X, div0(l,var_cols), var_cols, std_cols, mean_cols\n",
    "\n",
    "def shuffle_data(X, Y):\n",
    "    data = np.hstack((X, np.array(Y).reshape(len(Y), 1)))\n",
    "    np.random.shuffle(data) \n",
    "    return data[:,:X.shape[1]], data[:,X.shape[1]:].reshape(len(Y))\n",
    "\n",
    "def div0( a, b ):\n",
    "    \"\"\" ignore / 0, div0( [-1, 0, 1], 0 ) -> [0, 0, 0] \"\"\"\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        c = np.true_divide( a, b )\n",
    "        c[ ~ np.isfinite( c )] = 0  # -inf inf NaN\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "         509661 function calls (509605 primitive calls) in 3.172 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        4    0.000    0.000    0.000    0.000 :0(acquire)\n",
      "    21324    0.078    0.000    0.078    0.000 :0(append)\n",
      "    38418    0.141    0.000    0.141    0.000 :0(array)\n",
      "    38418    0.125    0.000    0.125    0.000 :0(dot)\n",
      "        1    0.000    0.000    3.172    3.172 :0(exec)\n",
      "        3    0.000    0.000    0.000    0.000 :0(getpid)\n",
      "        3    0.000    0.000    0.000    0.000 :0(isinstance)\n",
      "   191661    0.297    0.000    0.297    0.000 :0(len)\n",
      "       57    0.000    0.000    0.000    0.000 :0(max)\n",
      "     1260    0.047    0.000    0.078    0.000 :0(min)\n",
      "        2    0.000    0.000    0.000    0.000 :0(print)\n",
      "        1    0.000    0.000    0.000    0.000 :0(setprofile)\n",
      "        1    0.000    0.000    0.000    0.000 :0(tolist)\n",
      "        4    0.000    0.000    0.000    0.000 :0(urandom)\n",
      "        1    0.000    0.000    3.172    3.172 <string>:1(<module>)\n",
      "        1    0.000    0.000    3.172    3.172 decision_tree.py:13(fit)\n",
      "     1232    0.000    0.000    0.000    0.000 decision_tree_helper.py:123(<listcomp>)\n",
      "    18593    0.203    0.000    0.312    0.000 decision_tree_helper.py:125(<listcomp>)\n",
      "    18593    0.188    0.000    0.250    0.000 decision_tree_helper.py:126(<listcomp>)\n",
      "    39650    0.219    0.000    0.438    0.000 decision_tree_helper.py:145(get_columns)\n",
      "    39650    0.219    0.000    0.219    0.000 decision_tree_helper.py:147(<listcomp>)\n",
      "     57/1    0.188    0.003    3.172    3.172 decision_tree_helper.py:151(build_tree)\n",
      "       85    0.000    0.000    0.000    0.000 decision_tree_helper.py:176(<lambda>)\n",
      "    19825    0.031    0.000    0.031    0.000 decision_tree_helper.py:192(<lambda>)\n",
      "     1232    0.000    0.000    0.000    0.000 decision_tree_helper.py:194(<lambda>)\n",
      "       57    0.000    0.000    0.000    0.000 decision_tree_helper.py:28(__init__)\n",
      "      114    0.000    0.000    0.000    0.000 decision_tree_helper.py:54(dict_of_values)\n",
      "    19853    0.141    0.000    0.578    0.000 decision_tree_helper.py:71(divide_data)\n",
      "    19853    0.203    0.000    0.203    0.000 decision_tree_helper.py:89(<listcomp>)\n",
      "    19853    0.234    0.000    0.234    0.000 decision_tree_helper.py:90(<listcomp>)\n",
      "    19825    0.859    0.000    2.234    0.000 decision_tree_helper.py:97(gini_impurity)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:180(schedule)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:285(_is_master_process)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:298(_schedule_flush)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:361(write)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:89(_event_pipe)\n",
      "        1    0.000    0.000    3.172    3.172 profile:0(print(clf_dt.fit(X,y)); print())\n",
      "        0    0.000             0.000          profile:0(profiler)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:1062(_wait_for_tstate_lock)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:1104(is_alive)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:506(is_set)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from decision_tree import DecisionTree\n",
    "filename = 'data/SPECTF.dat'\n",
    "data = np.loadtxt(filename, delimiter=',')\n",
    "data.shape\n",
    "X = data[:, 1:]\n",
    "y = np.array(data[:, 0])\n",
    "clf_dt = DecisionTree(50)\n",
    "profile.run('print(clf_dt.fit(X,y)); print()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "         509661 function calls (509605 primitive calls) in 2.609 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        4    0.000    0.000    0.000    0.000 :0(acquire)\n",
      "    21324    0.078    0.000    0.078    0.000 :0(append)\n",
      "    38418    0.078    0.000    0.078    0.000 :0(array)\n",
      "    38418    0.125    0.000    0.125    0.000 :0(dot)\n",
      "        1    0.000    0.000    2.609    2.609 :0(exec)\n",
      "        3    0.000    0.000    0.000    0.000 :0(getpid)\n",
      "        3    0.000    0.000    0.000    0.000 :0(isinstance)\n",
      "   191661    0.328    0.000    0.328    0.000 :0(len)\n",
      "       57    0.000    0.000    0.000    0.000 :0(max)\n",
      "     1260    0.062    0.000    0.078    0.000 :0(min)\n",
      "        2    0.000    0.000    0.000    0.000 :0(print)\n",
      "        1    0.000    0.000    0.000    0.000 :0(setprofile)\n",
      "        1    0.000    0.000    0.000    0.000 :0(tolist)\n",
      "        4    0.000    0.000    0.000    0.000 :0(urandom)\n",
      "        1    0.000    0.000    2.609    2.609 <string>:1(<module>)\n",
      "        1    0.000    0.000    2.609    2.609 decision_tree.py:13(fit)\n",
      "     1232    0.000    0.000    0.000    0.000 decision_tree_helper.py:123(<listcomp>)\n",
      "    18593    0.234    0.000    0.312    0.000 decision_tree_helper.py:125(<listcomp>)\n",
      "    18593    0.094    0.000    0.188    0.000 decision_tree_helper.py:126(<listcomp>)\n",
      "    39650    0.172    0.000    0.422    0.000 decision_tree_helper.py:145(get_columns)\n",
      "    39650    0.250    0.000    0.250    0.000 decision_tree_helper.py:147(<listcomp>)\n",
      "     57/1    0.125    0.002    2.609    2.609 decision_tree_helper.py:151(build_tree)\n",
      "       85    0.000    0.000    0.000    0.000 decision_tree_helper.py:176(<lambda>)\n",
      "    19825    0.016    0.000    0.016    0.000 decision_tree_helper.py:192(<lambda>)\n",
      "     1232    0.000    0.000    0.000    0.000 decision_tree_helper.py:194(<lambda>)\n",
      "       57    0.000    0.000    0.000    0.000 decision_tree_helper.py:28(__init__)\n",
      "      114    0.000    0.000    0.000    0.000 decision_tree_helper.py:54(dict_of_values)\n",
      "    19853    0.188    0.000    0.578    0.000 decision_tree_helper.py:71(divide_data)\n",
      "    19853    0.188    0.000    0.188    0.000 decision_tree_helper.py:89(<listcomp>)\n",
      "    19853    0.203    0.000    0.203    0.000 decision_tree_helper.py:90(<listcomp>)\n",
      "    19825    0.469    0.000    1.750    0.000 decision_tree_helper.py:97(gini_impurity)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:180(schedule)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:285(_is_master_process)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:298(_schedule_flush)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:361(write)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:89(_event_pipe)\n",
      "        1    0.000    0.000    2.609    2.609 profile:0(print(clf_dt.fit(X,y)); print())\n",
      "        0    0.000             0.000          profile:0(profiler)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:1062(_wait_for_tstate_lock)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:1104(is_alive)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:506(is_set)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from decision_tree import DecisionTree\n",
    "filename = 'data/SPECTF.dat'\n",
    "data = np.loadtxt(filename, delimiter=',')\n",
    "data.shape\n",
    "X = data[:, 1:]\n",
    "y = np.array(data[:, 0])\n",
    "clf_dt = DecisionTree(50)\n",
    "profile.run('print(clf_dt.fit(X,y)); print()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=50,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\n",
      "         1067 function calls (940 primitive calls) in 0.016 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       20    0.000    0.000    0.000    0.000 :0(__subclasses__)\n",
      "       36    0.000    0.000    0.000    0.000 :0(_filters_mutated)\n",
      "        4    0.000    0.000    0.000    0.000 :0(_getframe)\n",
      "        4    0.000    0.000    0.000    0.000 :0(acquire)\n",
      "       28    0.000    0.000    0.000    0.000 :0(add)\n",
      "        1    0.000    0.000    0.000    0.000 :0(any)\n",
      "       40    0.000    0.000    0.000    0.000 :0(append)\n",
      "        1    0.000    0.000    0.000    0.000 :0(argsort)\n",
      "       15    0.000    0.000    0.000    0.000 :0(array)\n",
      "        1    0.000    0.000    0.000    0.000 :0(astype)\n",
      "        1    0.000    0.000    0.000    0.000 :0(build)\n",
      "        1    0.000    0.000    0.000    0.000 :0(callable)\n",
      "        2    0.000    0.000    0.000    0.000 :0(concatenate)\n",
      "        1    0.000    0.000    0.000    0.000 :0(cumsum)\n",
      "        1    0.000    0.000    0.000    0.000 :0(empty)\n",
      "        1    0.000    0.000    0.016    0.016 :0(exec)\n",
      "        2    0.000    0.000    0.000    0.000 :0(flatten)\n",
      "       14    0.000    0.000    0.000    0.000 :0(get)\n",
      "       31    0.000    0.000    0.000    0.000 :0(getattr)\n",
      "        3    0.000    0.000    0.000    0.000 :0(getpid)\n",
      "       13    0.000    0.000    0.000    0.000 :0(hasattr)\n",
      "        1    0.000    0.000    0.000    0.000 :0(id)\n",
      "       12    0.000    0.000    0.000    0.000 :0(insert)\n",
      "       13    0.000    0.000    0.000    0.000 :0(isidentifier)\n",
      "    94/50    0.000    0.000    0.000    0.000 :0(isinstance)\n",
      "    65/12    0.000    0.000    0.000    0.000 :0(issubclass)\n",
      "        1    0.000    0.000    0.000    0.000 :0(items)\n",
      "        1    0.000    0.000    0.000    0.000 :0(iter)\n",
      "        4    0.000    0.000    0.000    0.000 :0(join)\n",
      "       67    0.000    0.000    0.000    0.000 :0(len)\n",
      "        1    0.000    0.000    0.000    0.000 :0(max)\n",
      "       12    0.000    0.000    0.000    0.000 :0(pop)\n",
      "        2    0.000    0.000    0.000    0.000 :0(print)\n",
      "        4    0.000    0.000    0.000    0.000 :0(reduce)\n",
      "       26    0.000    0.000    0.000    0.000 :0(remove)\n",
      "       10    0.000    0.000    0.000    0.000 :0(repr)\n",
      "        1    0.000    0.000    0.000    0.000 :0(reshape)\n",
      "        5    0.000    0.000    0.000    0.000 :0(rstrip)\n",
      "        1    0.000    0.000    0.000    0.000 :0(setprofile)\n",
      "        1    0.016    0.016    0.016    0.016 :0(sort)\n",
      "        2    0.000    0.000    0.000    0.000 :0(sorted)\n",
      "        1    0.000    0.000    0.000    0.000 :0(split)\n",
      "        2    0.000    0.000    0.000    0.000 :0(sum)\n",
      "        4    0.000    0.000    0.000    0.000 :0(urandom)\n",
      "        1    0.000    0.000    0.000    0.000 :0(values)\n",
      "        1    0.000    0.000    0.000    0.000 :0(zeros)\n",
      "        1    0.000    0.000    0.016    0.016 <string>:1(<module>)\n",
      "       12    0.000    0.000    0.000    0.000 _collections_abc.py:302(__subclasshook__)\n",
      "        1    0.000    0.000    0.000    0.000 _methods.py:25(_amax)\n",
      "        2    0.000    0.000    0.000    0.000 _methods.py:31(_sum)\n",
      "        1    0.000    0.000    0.000    0.000 _methods.py:37(_any)\n",
      "       14    0.000    0.000    0.000    0.000 _weakrefset.py:16(__init__)\n",
      "       14    0.000    0.000    0.000    0.000 _weakrefset.py:20(__enter__)\n",
      "       14    0.000    0.000    0.000    0.000 _weakrefset.py:26(__exit__)\n",
      "       14    0.000    0.000    0.000    0.000 _weakrefset.py:36(__init__)\n",
      "       14    0.000    0.000    0.000    0.000 _weakrefset.py:52(_commit_removals)\n",
      "       47    0.000    0.000    0.000    0.000 _weakrefset.py:58(__iter__)\n",
      "       52    0.000    0.000    0.000    0.000 _weakrefset.py:70(__contains__)\n",
      "       14    0.000    0.000    0.000    0.000 _weakrefset.py:81(add)\n",
      "        4    0.000    0.000    0.000    0.000 abc.py:178(__instancecheck__)\n",
      "     31/1    0.000    0.000    0.000    0.000 abc.py:194(__subclasscheck__)\n",
      "        1    0.000    0.000    0.000    0.000 arrayprint.py:173(get_printoptions)\n",
      "        2    0.000    0.000    0.000    0.000 arrayprint.py:48(set_printoptions)\n",
      "        2    0.000    0.000    0.016    0.008 arraysetops.py:96(unique)\n",
      "        5    0.000    0.000    0.000    0.000 base.py:1081(isspmatrix)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:132(_pprint)\n",
      "        6    0.000    0.000    0.000    0.000 base.py:178(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:193(_get_param_names)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:207(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:218(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:220(get_params)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:295(__repr__)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:127(reshape)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:1900(any)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2067(cumsum)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2200(amax)\n",
      "        1    0.000    0.000    0.000    0.000 function_base.py:1264(copy)\n",
      "        2    0.000    0.000    0.000    0.000 inspect.py:158(isfunction)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2066(_signature_from_function)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2147(_signature_from_callable)\n",
      "       13    0.000    0.000    0.000    0.000 inspect.py:2400(__init__)\n",
      "       38    0.000    0.000    0.000    0.000 inspect.py:2449(name)\n",
      "       24    0.000    0.000    0.000    0.000 inspect.py:2461(kind)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2679(__init__)\n",
      "       14    0.000    0.000    0.000    0.000 inspect.py:2724(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2748(from_callable)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2754(parameters)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3000(signature)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:458(unwrap)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:478(_is_wrapper)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:180(schedule)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:285(_is_master_process)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:298(_schedule_flush)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:361(write)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:89(_event_pipe)\n",
      "        1    0.000    0.000    0.000    0.000 multiclass.py:113(is_multilabel)\n",
      "        1    0.000    0.000    0.016    0.016 multiclass.py:158(check_classification_targets)\n",
      "        1    0.000    0.000    0.016    0.016 multiclass.py:176(type_of_target)\n",
      "        2    0.000    0.000    0.000    0.000 numeric.py:414(asarray)\n",
      "        7    0.000    0.000    0.000    0.000 numeric.py:484(asanyarray)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:535(ascontiguousarray)\n",
      "        1    0.000    0.000    0.016    0.016 profile:0(print(clf.fit(X,y)); print())\n",
      "        0    0.000             0.000          profile:0(profiler)\n",
      "        1    0.000    0.000    0.000    0.000 shape_base.py:61(atleast_2d)\n",
      "        1    0.000    0.000    0.000    0.000 shape_base.py:9(atleast_1d)\n",
      "        1    0.000    0.000    0.000    0.000 six.py:437(iteritems)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:1062(_wait_for_tstate_lock)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:1104(is_alive)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:506(is_set)\n",
      "        1    0.000    0.000    0.016    0.016 tree.py:117(fit)\n",
      "        1    0.000    0.000    0.016    0.016 tree.py:698(fit)\n",
      "       16    0.000    0.000    0.000    0.000 typing.py:1010(__eq__)\n",
      "        8    0.000    0.000    0.000    0.000 typing.py:830(_valid_for_check)\n",
      "        6    0.000    0.000    0.000    0.000 typing.py:850(__extrahook__)\n",
      "        2    0.000    0.000    0.000    0.000 typing.py:865(__extrahook__)\n",
      "        2    0.000    0.000    0.000    0.000 validation.py:111(_num_samples)\n",
      "        2    0.000    0.000    0.000    0.000 validation.py:132(_shape_repr)\n",
      "        5    0.000    0.000    0.000    0.000 validation.py:159(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 validation.py:272(check_array)\n",
      "        2    0.000    0.000    0.000    0.000 validation.py:49(_assert_all_finite)\n",
      "        1    0.000    0.000    0.000    0.000 validation.py:565(check_random_state)\n",
      "       12    0.000    0.000    0.000    0.000 warnings.py:143(simplefilter)\n",
      "       12    0.000    0.000    0.000    0.000 warnings.py:159(_add_filter)\n",
      "       12    0.000    0.000    0.000    0.000 warnings.py:428(__init__)\n",
      "       12    0.000    0.000    0.000    0.000 warnings.py:449(__enter__)\n",
      "       12    0.000    0.000    0.000    0.000 warnings.py:468(__exit__)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=50)\n",
    "profile.run('print(clf.fit(X,y)); print()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_arr = np.array([[1.2, 'yes'], [5, 'no'], [7.8, 'yes']])\n",
    "np.dot(test_arr[:, 0].astype(float), test_arr[:, 0].astype(float))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
