{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import decision_tree_helper\n",
    "from decision_tree_helper import build_tree\n",
    "\n",
    "class DecisionTree(object):\n",
    "    \"\"\"\n",
    "    DecisionTree class, that represents one Decision Tree\n",
    "\n",
    "    :param max_tree_depth: maximum depth for this tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_tree_depth):\n",
    "        self.max_depth = max_tree_depth\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param Y: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \"\"\"\n",
    "        data = X.tolist()\n",
    "        for index in range(len(X)):\n",
    "            data[index].append(Y[index])\n",
    "        self.tree = build_tree(data, max_depth = self.max_depth)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :return: Y - 1 dimension python list with labels\n",
    "        \"\"\"\n",
    "        X = X.tolist()\n",
    "        Y = []\n",
    "        for row in X:\n",
    "            current_node = self.tree     \n",
    "            while(current_node != None and not current_node.is_leaf):\n",
    "                feature_val = row[current_node.column] \n",
    "                if type(feature_val) == int or type(feature_val) == float:\n",
    "                    if feature_val >= current_node.value:\n",
    "                        current_node = current_node.true_branch\n",
    "                    else: \n",
    "                        current_node = current_node.false_branch\n",
    "                if type(feature_val) == str:\n",
    "                    if feature_val == current_node.value:\n",
    "                        current_node = current_node.true_branch\n",
    "                    else:\n",
    "                        current_node = current_node.false_branch\n",
    "            Y.append(current_node.result)\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from decision_tree import DecisionTree\n",
    "\n",
    "class RandomForest(object):\n",
    "    \"\"\"\n",
    "    RandomForest a class, that represents Random Forests.\n",
    "\n",
    "    :param num_trees: Number of trees in the random forest\n",
    "    :param max_tree_depth: maximum depth for each of the trees in the forest.\n",
    "    :param ratio_per_tree: ratio of points to use to train each of\n",
    "        the trees.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_trees, max_tree_depth, ratio_per_tree=0.5):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_tree_depth = max_tree_depth\n",
    "        self.trees = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param Y: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        ind = np.arange(X.shape[0])\n",
    "        for _ in rang(num_trees):\n",
    "            train_ind = np.random.choice(ind, int(X.shape[0]*ratio_per_tree), replace=False)\n",
    "            tree_clf = DecisionTree(max_tree_depth)\n",
    "            tree_clf.fit(X[train_ind], Y[train_ind])\n",
    "            self.trees.append(tree_clf.tree)\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :return: (Y, conf), tuple with Y being 1 dimension python\n",
    "        list with labels, and conf being 1 dimensional list with\n",
    "        confidences for each of the labels.\n",
    "        \"\"\"\n",
    "        Y_list = []\n",
    "        for tree in self.trees:\n",
    "            Y_list.append(tree.predict(X))\n",
    "        Y = []\n",
    "        conf = []\n",
    "        for i in range(self.num_trees):\n",
    "            Y_i_counter = get_columns(Y_list, i)\n",
    "            y = max(Y_i_counter, key=lambda key: dict_current_results[key])\n",
    "            Y.append(y)\n",
    "            conf.append(Y_i_counter[y]/self.num_trees)\n",
    "        return (Y, conf)\n",
    "    \n",
    "    \n",
    "def get_columns(list_2D, cols):\n",
    "    if type(cols) == int or type(cols) == float:\n",
    "        return [item[cols] for item in list_2D]\n",
    "    return [[item[col] for col in cols] for item in list_2D]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration= 0, and cost_value= 1.440655453553484\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4456062717300813\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4505570899066784\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.440655453553484\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.460458726259873\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4555079080832756\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4357046353768863\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4555079080832756\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4703603626130675\n",
      "the gradient descent algorithm is finished.\n",
      "iteration= 0, and cost_value= 1.4555079080832756\n",
      "the gradient descent algorithm is finished.\n",
      "Decision Tree Accuracy =  0.669230769231  ( 0.0 )\n",
      "Random Forest Tree Accuracy =  0.0  ( 0.0 )\n",
      "Logistic Reg. Accuracy =  0.765384615385  ( 0.0 )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from logistic_regression import LogisticRegression\n",
    "from decision_tree import DecisionTree\n",
    "\n",
    "def accuracy_score(Y_true, Y_predict):\n",
    "    tp_plus_tn = 0\n",
    "    N = len(Y_true)\n",
    "    for index in range(N):\n",
    "        if Y_true[index] == Y_predict[index]:\n",
    "            tp_plus_tn += 1\n",
    "    return tp_plus_tn / N\n",
    "\n",
    "\n",
    "def evaluate_performance():\n",
    "    '''\n",
    "    Evaluate the performance of decision trees and logistic regression,\n",
    "    average over 1,000 trials of 10-fold cross validation\n",
    "\n",
    "    Return:\n",
    "      a matrix giving the performance that will contain the following entries:\n",
    "      stats[0,0] = mean accuracy of decision tree\n",
    "      stats[0,1] = std deviation of decision tree accuracy\n",
    "      stats[1,0] = mean accuracy of logistic regression\n",
    "      stats[1,1] = std deviation of logistic regression accuracy\n",
    "\n",
    "    ** Note that your implementation must follow this API**\n",
    "    '''\n",
    "\n",
    "    # Load Data\n",
    "    filename = 'data/SPECTF.dat'\n",
    "    data = np.loadtxt(filename, delimiter=',')\n",
    "    X = data[:, 1:]\n",
    "    y = np.array(data[:, 0])\n",
    "    n, d = X.shape\n",
    "\n",
    "    all_accuracies_dt = []\n",
    "    all_accuracies_lr = []\n",
    "    for trial in range(1):\n",
    "        idx = np.arange(n)\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "        \n",
    "        ind = np.arange(X.shape[0])\n",
    "        classifier_dt = DecisionTree(50)\n",
    "        classifier_lr = LogisticRegression(max_steps=10000, epsilon=1e-7)\n",
    "        scores_dt = []\n",
    "        scores_lr = []\n",
    "        for i in range(10):\n",
    "            test_ind = np.random.choice(ind, int(X.shape[0]/10), replace=False)\n",
    "            ind = np.setdiff1d(np.arange(X.shape[0]), test_ind)\n",
    "            X_train, Y_train, X_test, Y_test = X[ind], y[ind], X[test_ind], y[test_ind]\n",
    "            # train the decision tree\n",
    "            classifier_dt.fit(X_train, Y_train)\n",
    "            accuracy_dt = accuracy_score(Y_true=Y_test, Y_predict=classifier_dt.predict(X_test))\n",
    "            scores_dt.append(accuracy_dt)\n",
    "            classifier_lr.fit(X_train, Y_train)\n",
    "            accuracy_lr = accuracy_score(Y_true=Y_test, Y_predict=classifier_lr.predict(X_test))\n",
    "            scores_lr.append(accuracy_lr)\n",
    "        all_accuracies_dt.append(np.mean(scores_dt))\n",
    "        all_accuracies_lr.append(np.mean(scores_lr))\n",
    "       \n",
    "\n",
    "\n",
    "    # compute the training accuracy of the model\n",
    "    meanDecisionTreeAccuracy = np.mean(all_accuracies_dt)\n",
    "    stddevDecisionTreeAccuracy = np.std(all_accuracies_dt)\n",
    "    # TODO: update these statistics based on the results of your experiment\n",
    "    meanLogisticRegressionAccuracy = np.mean(all_accuracies_lr)\n",
    "    stddevLogisticRegressionAccuracy = np.std(all_accuracies_lr)\n",
    "    meanRandomForestAccuracy = 0\n",
    "    stddevRandomForestAccuracy = 0\n",
    "\n",
    "    # make certain that the return value matches the API specification\n",
    "    stats = np.zeros((3, 2))\n",
    "    stats[0, 0] = meanDecisionTreeAccuracy\n",
    "    stats[0, 1] = stddevDecisionTreeAccuracy\n",
    "    stats[1, 0] = meanRandomForestAccuracy\n",
    "    stats[1, 1] = stddevRandomForestAccuracy\n",
    "    stats[2, 0] = meanLogisticRegressionAccuracy\n",
    "    stats[2, 1] = stddevLogisticRegressionAccuracy\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Do not modify from HERE...\n",
    "if __name__ == \"__main__\":\n",
    "    stats = evaluate_performance()\n",
    "    print(\"Decision Tree Accuracy = \", stats[0, 0], \" (\", stats[0, 1], \")\")\n",
    "    print(\"Random Forest Tree Accuracy = \", stats[1, 0], \" (\", stats[1, 1], \")\")\n",
    "    print(\"Logistic Reg. Accuracy = \", stats[2, 0], \" (\", stats[2, 1], \")\")\n",
    "# ...to HERE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \n",
    "    def __init__(self, epsilon=0.0001, l=1, step_size=0.01, max_steps=1000, initial_beta=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.l = l\n",
    "        self.step_size = step_size\n",
    "        self.max_steps = max_steps\n",
    "        self.initial_beta = initial_beta\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param Y: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \"\"\"\n",
    "        self.beta = stochastic_gradient_descent(X = X, \n",
    "                                                Y = Y, \n",
    "                                                epsilon=self.epsilon, \n",
    "                                                l=self.l, \n",
    "                                                step_size=self.step_size, \n",
    "                                                max_steps=self.max_steps,\n",
    "                                               initial_beta=self.initial_beta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :return: Y - 1 dimension python list with labels\n",
    "        \"\"\"\n",
    "        Y = []\n",
    "        for row in X:\n",
    "            Y.append(int(np.dot(row, beta) >= 0))\n",
    "        return Y\n",
    "\n",
    "def sigmoid(s):\n",
    "    return 1/(1+np.exp(-s))\n",
    "\n",
    "def cost_function(X, Y, beta):\n",
    "    h = sigmoid(np.dot(X, beta))\n",
    "    return (-np.dot(Y, np.log(h))+ np.dot((1+Y),1-h))/X.shape[0]\n",
    "\n",
    "def stochastic_gradient_descent(X, Y, epsilon=0.0001, l=1, step_size=0.01, max_steps=1000, initial_beta=None):\n",
    "    \"\"\"\n",
    "    Implement gradient descent using stochastic approximation of the gradient.\n",
    "    :param X: data matrix (2 dimensional np.array)\n",
    "    :param Y: response variables (1 dimensional np.array)\n",
    "    :param l: regularization parameter lambda\n",
    "    :param epsilon: approximation strength\n",
    "    :param max_steps: maximum number of iterations before algorithm will\n",
    "        terminate.\n",
    "    :return: value of beta (1 dimensional np.array)\n",
    "    \"\"\"\n",
    "    N, D = X.shape[0], X.shape[1]\n",
    "    X, l_vector, var_cols, std_cols, mean_cols = parameters_for_scaling(X, l)\n",
    "    l_vector[0] = 0\n",
    "    if initial_beta == None:\n",
    "        beta = np.zeros(D)\n",
    "    else:\n",
    "        beta = initial_beta\n",
    "    for s in range(max_steps):\n",
    "        if s % N == 0:\n",
    "            X, Y = shuffle_data(X, Y)\n",
    "        next_beta = beta - step_size*normalized_gradient(X[s%N], Y[s%N], beta, l_vector)\n",
    "        if s % 1000 == 0:\n",
    "            print('iteration= {}, and cost_value= {}'.format(s, cost_function(X, Y, beta)))\n",
    "        if np.linalg.norm(next_beta - beta)/np.linalg.norm(next_beta) < epsilon:\n",
    "            print('the gradient descent algorithm is finished.')\n",
    "            return get_real_beta(next_beta, std_cols, mean_cols)\n",
    "        beta = next_beta\n",
    "    print('the gradient descent algorithm is finished.')\n",
    "    return get_real_beta(beta, std_cols, mean_cols)\n",
    "\n",
    "\n",
    "\n",
    "def normalized_gradient(X, Y, beta, l):\n",
    "    \"\"\"\n",
    "    :param X: data matrix (2 dimensional np.array)\n",
    "    :param Y: response variables (1 dimensional np.array)\n",
    "    :param beta: value of beta (1 dimensional np.array)\n",
    "    :param l: regularization parameter lambda\n",
    "    :return: normalized gradient, i.e. gradient normalized according to data\n",
    "    \"\"\"\n",
    "    return (np.dot(X.T, sigmoid(np.dot(X,beta))-Y) + l*beta/2)/X.shape[0]\n",
    "\n",
    "def parameters_for_scaling(X, l):\n",
    "    X = np.copy(X)\n",
    "    featchures_mat = X[:, 1:]\n",
    "    var_cols = np.var(X, axis=0)\n",
    "    std_cols = np.std(X, axis=0)\n",
    "    mean_cols = np.mean(X, axis=0)\n",
    "    var_cols[0] = 1\n",
    "    featchures_mat = X[:, 1:]\n",
    "    X[:, 1:] = div0(featchures_mat - np.mean(featchures_mat, axis=0),np.std(featchures_mat, axis=0))\n",
    "    return X, div0(l,var_cols), var_cols, std_cols, mean_cols\n",
    "\n",
    "def shuffle_data(X, Y):\n",
    "    data = np.hstack((X, np.array(Y).reshape(len(Y), 1)))\n",
    "    np.random.shuffle(data) \n",
    "    return data[:,:X.shape[1]], data[:,X.shape[1]:].reshape(len(Y))\n",
    "\n",
    "def div0( a, b ):\n",
    "    \"\"\" ignore / 0, div0( [-1, 0, 1], 0 ) -> [0, 0, 0] \"\"\"\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        c = np.true_divide( a, b )\n",
    "        c[ ~ np.isfinite( c )] = 0  # -inf inf NaN\n",
    "    return c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
